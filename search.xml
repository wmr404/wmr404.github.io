<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>胶囊间的动态路由-论文解读</title>
      <link href="/2020/03/01/capsule/"/>
      <url>/2020/03/01/capsule/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Capsule介绍"><a href="#1-Capsule介绍" class="headerlink" title="1. Capsule介绍"></a>1. Capsule介绍</h1><blockquote><p>Sabour, Sara, Nicholas Frosst, and Geoffrey E. Hinton. “Dynamic routing between capsules.” Advances in neural information processing systems. 2017.</p></blockquote><p>Capsule特色是“vector in vector out”，取代了以往的“scaler in scaler out”，也就是神经元的输入输出都变成了向量，从而算是对神经网络理论的一次革命。</p><p>然而在目前的深度学习中，从来不缺乏“vector in vector out”的案例，因此显然这不能算是Capsule的革命。比如在NLP中，一个词向量序列的输入模型，这个词向量序列再经过RNN/CNN/Attention的编码，输出一个新序列，不也是“vector in vector out”吗？</p><p>Capsule的革命在于：<strong>它提出了一种新的“vector in vector out”的传递方案，并且这种方案在很大程度上是可解释的。</strong></p><p>深度学习（神经网络）为什么有效：神经网络通过层层叠加完成了对输入的层层抽象，这个过程某种程度上<strong>模拟了人的层次分类做法</strong>，从而完成对最终目标的输出，并且具有比较好的泛化能力。的确，神经网络应该是这样做的，然而它并不能告诉我们它确确实实是这样做的，这就是神经网络的难解释性，也就是很多人会将深度学习视为黑箱的原因之一。</p><p>下面介绍Capsule是怎么突破这一点的。</p><h1 id="2-CapsNet模型"><a href="#2-CapsNet模型" class="headerlink" title="2. CapsNet模型"></a>2. CapsNet模型</h1><p>CapsNet: 两个卷积层(Conv 1, PrimaryCaps)，一个全连接层(DigitCaps)</p><p><img src="https://img-blog.csdnimg.cn/20190615192127397.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="2-1-Conv1层"><a href="#2-1-Conv1层" class="headerlink" title="2.1 Conv1层"></a>2.1 Conv1层</h2><p>常规的卷积层,  起像素级局部特征检测作用</p><p><img src="https://img-blog.csdnimg.cn/20190615192144318.png" alt="在这里插入图片描述"><br>$shape: [None,28,28,1] \rightarrow [None,20,20,256]$</p><h2 id="2-2-PrimaryCaps层"><a href="#2-2-PrimaryCaps层" class="headerlink" title="2.2 PrimaryCaps层"></a>2.2 PrimaryCaps层</h2><p>生成最低级卷积8D<strong>胶囊</strong>层（无路由）</p><p>胶囊：其实，<strong>只要把一个向量当作一个整体来看，它就是一个“胶囊”。可以这样理解：神经元就是标量，胶囊就是向量</strong>。Hinton的理解是：每一个胶囊表示一个属性，而胶囊的向量则表示这个属性的“标架”。也就是说，我们以前只是用一个标量表示有没有这个特征（比如有没有羽毛），现在我们用一个向量来表示，不仅仅表示有没有，还表示“有什么样的”（比如有什么颜色、什么纹理的羽毛），如果这样理解，就是说在对单个特征的表达上更丰富了。</p><p>简单的讲，PrimaryCaps层要输出一些8D的向量，每个向量代表一些比较低级的特征，向量的各个位置的值代表该特征的属性</p><p><strong>PrimaryCaps层: 计算过程具有多种理解方式，其中之一为，8个并行的常规卷积层的叠堆</strong></p><h3 id="PrimaryCaps层的第一种理解方式"><a href="#PrimaryCaps层的第一种理解方式" class="headerlink" title="PrimaryCaps层的第一种理解方式"></a>PrimaryCaps层的第一种理解方式</h3><p>8个并行的常规卷积层:<br><img src="https://img-blog.csdnimg.cn/20190615192158370.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>卷积操作2参数:<br><img src="https://img-blog.csdnimg.cn/20190615192208901.png" alt="在这里插入图片描述"><br>对Conv1层的输出进行8次卷积操作2所示的卷积操作：<br><img src="https://img-blog.csdnimg.cn/20190615192232750.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>然后对8个并行常规卷积层叠堆（对每个卷积层的各个通道在第四个维度上进行合并）：</p><p>8个[6,6,1,32]卷积层合并示意图如下:<br><img src="https://img-blog.csdnimg.cn/20190615192257910.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>得到叠堆后的结果：[None,6,6,8,32]<br>然后展开为[None,6x6x32,8,1]=[None,1152,8,1]，这样我们就得到了1152个初始胶囊，每个胶囊是一个纬度为[8,1]的向量，并代表某一特征</p><h3 id="PrimaryCaps层的第二种理解方式"><a href="#PrimaryCaps层的第二种理解方式" class="headerlink" title="PrimaryCaps层的第二种理解方式"></a>PrimaryCaps层的第二种理解方式</h3><ul><li>32个通道之间的卷积核是独立的(9x9大小)</li><li>8个并行卷积层之间的参数也是独立的</li></ul><p>$Rightarrow$ 即共有8x32个大小为9x9的相互独立的卷积核，可看作8x32个通道的常规卷积和<br>则可以用下面的操作得到和第一种理解方式相同的结果<br><img src="https://img-blog.csdnimg.cn/2019061519232534.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>输出：$[None,6,6,32*8] \overset{reshape}{\rightarrow} [None,1152,8,1])$</p><p><strong>注意</strong>：虽然计算方式上与常规卷积层无差异，但意义上却已经大不相同！将达到8x1capsule的特征封装的效果</p><p><strong>PrimaryCaps输出输出1152个8D的胶囊后，使用了一个Squash函数做非线性变换，那么，为什么要设计这个函数呢？这个函数的原理是啥</strong></p><h2 id="2-3-Squash函数"><a href="#2-3-Squash函数" class="headerlink" title="2.3 Squash函数"></a>2.3 Squash函数</h2><h3 id="为什么要设计squash函数："><a href="#为什么要设计squash函数：" class="headerlink" title="为什么要设计squash函数："></a>为什么要设计squash函数：</h3><p>因为论文希望Capsule能有一个性质：<strong>胶囊的模长能够代表这个特征的概率，即特征的“显著程度”， 模长越大，这个特征越显著，而我们又希望有一个有界的指标来对这个“显著程度”进行衡量，所以就只能对这个模长进行压缩了</strong></p><h3 id="squash函数的原理："><a href="#squash函数的原理：" class="headerlink" title="squash函数的原理："></a>squash函数的原理：</h3><p>$$squash(x) = \frac{||x||^2}{1+||x||^2} \frac{x}{||x||}$$</p><ul><li>$\frac{x}{||x||}$: 将x的模长变为1</li><li>$\frac{||x||^2}{1+||x||^2}$:     起缩放x的模长的作用，x模长越大，$\frac{||x||^2}{1+||x||^2}$越趋近于1，||x||=0时，$\frac{||x||^2}{1+||x||^2}=0$</li></ul><p>则$y=squash(x)$的效果为：x的模长越大，y的模长越趋近于1</p><p><strong>PrimaryCaps输出的1152个8D的胶囊经过squash函数后非线性变换后，都具有了胶囊的模长能够代表这个特征的概率的特性，这些新的胶囊接着作为DigitCaps层的输入。</strong></p><h2 id="2-4-DigitCaps层"><a href="#2-4-DigitCaps层" class="headerlink" title="2.4 DigitCaps层"></a>2.4 DigitCaps层</h2><p>由于该层解释起来比较复杂，所以先从简单例子开始，慢慢推出该层的流程及原理。</p><h3 id="capsule示意"><a href="#capsule示意" class="headerlink" title="capsule示意"></a>capsule示意</h3><p>capsule示意图:</p><p><img src="https://img-blog.csdnimg.cn/20190615192340940.png" alt="在这里插入图片描述"><br>如上图所示，底层的胶囊和高层的胶囊构成一些连接关系那么，这些胶囊要怎么运算，才能体现出“层层抽象”、“层层分类”的特性呢？让我们先看其中一部分连接：</p><p><img src="https://img-blog.csdnimg.cn/20190615192354980.png" alt="在这里插入图片描述"><br>u,v都是胶囊，图上只展示了$u_1$的连接。这也就是说，目前已经有了$u_1$这个特征（假设是羽毛），那么我想知道它属于上层特征$v_1,v_2  ,v_3,v_4$（假设分别代表了鸡、鸭、鱼、狗）中的哪一个。分类问题我们显然已经是很熟悉了，不就是内积后softmax吗？于是单靠$u_1$这个特征，我们推导出它是属于鸡、鸭、鱼、狗的概率分别是:</p><p>$$(p_{1|1},p_{2|1},p_{3|1},p_{4|1})=\frac{1}{Z_1} (e^{&lt;u_1,v_1&gt;},e^{&lt;u_1,v_2&gt;},e^{&lt;u_1,v_3&gt;},e^{&lt;u_1,v_4&gt;})$$</p><p>我们期望$p_{1|1}，p_{2|1}$会明显大于$p_{3|1}，p_{4|1}$（鸡鸭有羽毛，鱼狗没羽毛）不过，单靠这个特征还不够，我们还需要综合各个特征，于是可以把上述操作对各个u_i都做一遍，继而得到<br>$$(p_{1|2},p_{2|2},p_{3|2},p_{4|2}), (p_{1|3},p_{2|3},p_{3|3},p_{4|3}), …$$</p><p>问题是，现在得到这么多预测结果，究竟要选择哪个呢？而且又不是真的要做分类，我们要的是融合这些特征，构成更高级的特征。</p><p>于是Hinton认为，既然$u_i$这个特征得到的概率分布是$(p_{1|i},p_{2|i},p_{3|i},p_{4|i})$那么我把这个特征切成四份，分别为$(p_{1|i}u_i,p_{2|i}u_i,p_{3|i}u_i,p_{4|i}u_i)$, 然后把这几个特征分别传给$v_1,v_2,v_3,v_4$，最后$v_1,v_2,v_3,v_4$其实就是底层传入的特征的累加</p><p>$$v_j=squash(p_{j|i} u_i )=squash(\sum_{i} \frac{e^{&lt;u_i,v_j&gt;}}{Z_i} u_i)$$</p><p>从上往下看，那么Capsule就是每个底层特征分别做分类，然后将分类结果整合。这时$v_j$应该尽量与所有$u_i$都比较靠近，靠近的度量是内积。因此，从下往上看的话，可以认为$v_j$实际上就是各个$u_i$的某个聚类中心，而Capsule的核心思想就是<strong>输出是输入的某种聚类结果。</strong></p><h3 id="动态路由："><a href="#动态路由：" class="headerlink" title="动态路由："></a>动态路由：</h3><p>注意到式子$v_j=squash(\sum_{i} \frac{e^{&lt;u_i,v_j&gt;}}{Z_i} u_i)$，为了求$v_j$需要求softmax，可是为了求softmax又需要知道$v_j$，这不是个鸡生蛋、蛋生鸡的问题了吗？而“动态路由”正是为了解决这一问题而提出的，它能够根据自身的特性来更新（部分）参数，从而初步达到了Hinton的放弃梯度下降的目标</p><p>下面通过几个例子来解释动态路由的过程：</p><h3 id="例1"><a href="#例1" class="headerlink" title="例1:"></a>例1:</h3><p>让我们先回到普通的神经网络，大家知道，激活函数在神经网络中的地位是举足轻重的。当然，激活函数本身很简单，比如一个tanh激活的全连接层。</p><p>可是，如果想用$x=y+cos⁡y$的反函数来激活呢？也就是说，得解出$y=f(x)$，然后再用它来做激活函数。<br>然而$x=y+cos⁡y$的反函数是一个超越函数，也就是不可能用初等函数有限地表示出来。<br>但我们可以通过迭代法求出y：<br>$$y_{n+1} = x - cos y_n$$</p><p>选择$y_0=x$，代入上式迭代几次，基本上就可以得到比较准确的y了。假如迭代3次，那就是<br>$$y=x-cos⁡(x-cos⁡(x-cos⁡x))$$<br>可以发现这和动态路由的过程有点像</p><h3 id="例2"><a href="#例2" class="headerlink" title="例2:"></a>例2:</h3><p>再来看一个例子，这个例子可能在NLP中有很多对应的情景，但图像领域其实也不少。考虑一个向量序列$(x_1,x_2,…,x_n)$，我现在要想办法将这n个向量整合成一个向量x（encoder），然后用这个向量来做分类：<br>$$x= \sum_{i=1}^n \lambda _i x_i$$<br>这里的λ_i相当于衡量了x与x_i的相似度。那么，在x出现之前，凭什么能够确定这个相似度呢？<br>解决这个问题的一个方案也是迭代。首先我们也可以定义一个基于softmax的相似度指标，然后让</p><p>$$x= \sum_{i=1}^n \frac{e^{&lt;x,x_i&gt;}}{Z} x_i$$</p><p>一开始，我们一无所知，所以只好取x为各个$x_i$的均值，然后代入右边就可以算出一个x，再把它代入右边，反复迭代就行，一般迭代有限次就可以收敛，于是就可以将这个迭代过程嵌入到神经网络中了。</p><p>如果说例1跟动态路由只是神似，那么例2已经跟动态路由是神似＋形似了。</p><p>通过例1，例2，已经可以很清晰的开始解释动态路由过程了<br>为了得到各个v_j，一开始先让它们全都等于u_i的均值，然后反复迭代就好。说白了，输出是输入的聚类结果，而聚类通常都需要迭代算法，这个迭代算法就称为“动态路由”。</p><p>到此，就可以写出论文里的动态路由的算法了：</p><hr><p>动态路由算法<br>初始化$b_{ij}$=0<br>迭代r次：<br>&nbsp;&nbsp;&nbsp;&nbsp;$c_i \leftarrow softmax(b_i)$<br>&nbsp;&nbsp;&nbsp;&nbsp;$s_j \leftarrow \sum_i c_{ij} u_i$<br>&nbsp;&nbsp;&nbsp;&nbsp;$v_j \leftarrow squash(s_j)$<br>&nbsp;&nbsp;&nbsp;&nbsp;$b_{ij} \leftarrow b_{ij}  + &lt;u_i,v_j&gt;$<br>返回$v_j$</p><hr><p>这里的$c_{ij}$就是前文的$p_{j|i}$前面已经说了，$v_j$是作为输入$u_i$的某种聚类中心出现的，而从不同角度看输入，得到的聚类结果显然是不一样的。那么为了实现“多角度看特征”，于是可以在每个胶囊传入下一个胶囊之前，都要先乘上一个矩阵做变换，所以式$v_j=squash(\sum_{i} \frac{e^{&lt;u_i,v_j&gt;}}{Z_i} u_i)$实际上应该要变为</p><p>$v_j=squash(\sum_{i} \frac{e^{&lt;u_i,v_j&gt;}}{Z_i} \hat{u} _{j|i})$</p><p>$\hat{u}<em>{j|i} = W</em>{ji} u_i$</p><p>这里的$W_{ji}$是待训练的矩阵，这里的乘法是矩阵乘法，也就是矩阵乘以向量。所以，Capsule变成了下图<br><img src="https://img-blog.csdnimg.cn/20190615192705864.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这时候就可以得到完整动态路由了:</p><hr><p>动态路由算法<br>初始化$b_{ij}$=0<br>迭代r次：<br>&nbsp;&nbsp;&nbsp;&nbsp;$c_i \leftarrow softmax(b_i)$<br>&nbsp;&nbsp;&nbsp;&nbsp;$s_j \leftarrow \sum_i c_{ij} \hat{u}<em>{j|i}$<br>&nbsp;&nbsp;&nbsp;&nbsp;$v_j \leftarrow squash(s_j)$<br>&nbsp;&nbsp;&nbsp;&nbsp;$b</em>{ij} \leftarrow b_{ij}  + &lt;u_i,v_j&gt;$<br>返回$v_j$</p><hr><p>这样的Capsule层，显然相当于普通神经网络中的全连接层。 </p><h3 id="DigitCaps层流程总结"><a href="#DigitCaps层流程总结" class="headerlink" title="DigitCaps层流程总结"></a>DigitCaps层流程总结</h3><ol><li>将PrimaryCaps输入的1152个8D的胶囊从乘$W_{ji}$，以达到不同角度看输入的目的，得到[None,10,1152,16,1]</li><li>对每个1152里面的16D的胶囊，通过动态路由算法聚类出一个$v_j$</li><li>返回[None,10,16],即输出10个胶囊，分别对应数字0～9，胶囊的模长代表是该数字的概率，每个胶囊内部的值代表了该数字的某一属性</li></ol><h2 id="2-5-重构层"><a href="#2-5-重构层" class="headerlink" title="2.5 重构层"></a>2.5 重构层</h2><p>重构层就比较简单了，但是也有一些细节需要说明一下<br><img src="https://img-blog.csdnimg.cn/20190615192716874.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>网络很简单，就是三个全连接层，其中有个masked操作，具体原理如下：<br>因为DigitCaps输出[None,10,16]，即每个样本输出10个16D的胶囊，胶囊的模长代表图片是这个类别的概率，而每个16D的胶囊里面各个位置的值则代表了这个数字的一系列属性，重构是该胶囊已经包含了大部分的信息，假设要重构的是数字5，那么就把DigitCaps该位置的mask设置为1，其他位置为0，那么实际重构事，就只有这个胶囊的信息参与了运算。</p><h2 id="2-6-损失函数Margin-loss-recon-loss"><a href="#2-6-损失函数Margin-loss-recon-loss" class="headerlink" title="2.6 损失函数Margin loss + recon loss"></a>2.6 损失函数Margin loss + recon loss</h2><h3 id="Margin-loss函数："><a href="#Margin-loss函数：" class="headerlink" title="Margin loss函数："></a>Margin loss函数：</h3><p>$$L_c = T_c max⁡(0,m^+ - ||v_c ||)^2+ \lambda (1-T_c )max⁡(0,||v_c||-m^- )^2$$</p><ul><li>$c$:类别</li><li>$T_c$:指示函数（分类c存在为1，否则为0）</li><li>$m^-$:$||v_c ||$上边界，避免假阴性，遗漏实际预测到存在的分类的情况</li><li>$m^+$:$||v_c |)|$下边界，避免假阳性</li><li>margin loss: $\sum_c L_c$ </li></ul><h3 id="重构误差："><a href="#重构误差：" class="headerlink" title="重构误差："></a>重构误差：</h3><ul><li>作用： 正则化</li><li>重构网络： MLP</li><li>重构误差计算方式MSE</li></ul><h1 id="3-运行结果"><a href="#3-运行结果" class="headerlink" title="3. 运行结果"></a>3. 运行结果</h1><h2 id="3-1测试集分类结果"><a href="#3-1测试集分类结果" class="headerlink" title="3.1测试集分类结果"></a>3.1测试集分类结果</h2><video src="/medias/capsule.mp4" controls="controls" width="640" height="320" autoplay="autoplay">Your browser does not support the video tag.</video><h3 id="论文分类结果"><a href="#论文分类结果" class="headerlink" title="论文分类结果:"></a>论文分类结果:</h3><p><img src="https://img-blog.csdnimg.cn/201906151927271.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="复现"><a href="#复现" class="headerlink" title="复现"></a>复现</h3><p>（routing:3,Reconstruction:yes）结果在测试集的准确率平均可达99.24%以上，基本复现成功</p><h2 id="3-2-单数字重构效果"><a href="#3-2-单数字重构效果" class="headerlink" title="3.2 单数字重构效果"></a>3.2 单数字重构效果</h2><h3 id="论文单数字重构效果"><a href="#论文单数字重构效果" class="headerlink" title="论文单数字重构效果:"></a>论文单数字重构效果:</h3><p><img src="https://img-blog.csdnimg.cn/20190615192737677.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="复现单数字重构效果"><a href="#复现单数字重构效果" class="headerlink" title="复现单数字重构效果:"></a>复现单数字重构效果:</h3><p><img src="https://img-blog.csdnimg.cn/20190615192749601.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="3-3-重叠数字重构效果"><a href="#3-3-重叠数字重构效果" class="headerlink" title="3.3 重叠数字重构效果"></a>3.3 重叠数字重构效果</h2><h3 id="论文重叠数字重构效果"><a href="#论文重叠数字重构效果" class="headerlink" title="论文重叠数字重构效果:"></a>论文重叠数字重构效果:</h3><p><img src="https://img-blog.csdnimg.cn/20190615192757872.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="复现重叠数字重构效果"><a href="#复现重叠数字重构效果" class="headerlink" title="复现重叠数字重构效果:"></a>复现重叠数字重构效果:</h3><p>第一行为实际图片和标签，第二行为预测的图片和标签，第三四行是把第二行两个图片分开的结果<br><img src="https://img-blog.csdnimg.cn/20190615192814738.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h1 id="4-github源码地址"><a href="#4-github源码地址" class="headerlink" title="4. github源码地址"></a>4. github源码地址</h1><blockquote><p><a href="https://github.com/wangjiosw/capsule-pytorch" target="_blank" rel="noopener">https://github.com/wangjiosw/capsule-pytorch</a></p></blockquote><h1 id="5-参考文章"><a href="#5-参考文章" class="headerlink" title="5. 参考文章"></a>5. 参考文章</h1><p>[1] <a href="https://kexue.fm/archives/4819" target="_blank" rel="noopener">揭开迷雾，来一顿美味的Capsule盛宴</a></p><p>[2] <a href="https://kexue.fm/archives/5112" target="_blank" rel="noopener">再来一顿贺岁宴：从K-Means到Capsule</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cpasule </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mac OS VMware Fusion Centos6.5虚拟机网络设置</title>
      <link href="/2020/02/29/vmware/"/>
      <url>/2020/02/29/vmware/</url>
      
        <content type="html"><![CDATA[<h1 id="1-安装vmware虚拟机"><a href="#1-安装vmware虚拟机" class="headerlink" title="1. 安装vmware虚拟机"></a>1. 安装vmware虚拟机</h1><p>安装vmware虚拟机，并新建一个centos 64位的虚拟机</p><h1 id="2-设置虚拟机网络模式"><a href="#2-设置虚拟机网络模式" class="headerlink" title="2. 设置虚拟机网络模式"></a>2. 设置虚拟机网络模式</h1><p><img src="https://img-blog.csdnimg.cn/20200210222413551.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="3-查看vmware的网关和掩码"><a href="#3-查看vmware的网关和掩码" class="headerlink" title="3. 查看vmware的网关和掩码"></a>3. 查看vmware的网关和掩码</h1><p>在Mac电脑的终端输入：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">cat</span> /Library/Preferences/VMware\ Fusion/vmnet8/nat.conf</code></pre><p>输出结果如下：<br><img src="https://img-blog.csdnimg.cn/20200210222624261.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt=""><br>这里的ip和netmask即为vmware虚拟机的网关和掩码</p><pre class=" language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># NAT gateway address</span>ip <span class="token operator">=</span> 172.16.143.2netmask <span class="token operator">=</span> 255.255.255.0</code></pre><p>ip和netmask后面配置centos虚拟机的网络时分别对于网关和掩码。</p><h1 id="4-配置centos虚拟机的网络"><a href="#4-配置centos虚拟机的网络" class="headerlink" title="4. 配置centos虚拟机的网络"></a>4. 配置centos虚拟机的网络</h1><p>在centos虚拟机的终端输入：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">vi</span> /etc/sysconfig/network-scripts/ifcfg-eth0</code></pre><p>然后：</p><ul><li>删除UUID和MAC地址</li><li>ONBOOT=yes</li><li>BOOTPROTO=static</li><li>IPADDR=172.16.143.101</li><li>NETMASK=255.255.255.0</li><li>GATEWAY=172.16.143.2</li><li>DNS1=172.16.143.2</li></ul><p>保存并退出，然后在centos虚拟机的终端输入：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">service</span> network restart</code></pre><p>ping 一下百度看是否能ping通：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">ping</span> www.baidu.com</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> vmware </tag>
            
            <tag> macos </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Torchtext使用教程</title>
      <link href="/2020/02/29/torchtext-use/"/>
      <url>/2020/02/29/torchtext-use/</url>
      
        <content type="html"><![CDATA[<h1 id="Torchtext使用教程"><a href="#Torchtext使用教程" class="headerlink" title="Torchtext使用教程"></a>Torchtext使用教程</h1><h2 id="主要内容："><a href="#主要内容：" class="headerlink" title="主要内容："></a>主要内容：</h2><ul><li>如何使用torchtext建立语料库</li><li>如何使用torchtext将词转下标，下标转词，词转词向量</li><li>如何建立相应的迭代器</li></ul><h2 id="torchtext预处理流程："><a href="#torchtext预处理流程：" class="headerlink" title="torchtext预处理流程："></a>torchtext预处理流程：</h2><ol><li>定义Field：声明如何处理数据</li><li>定义Dataset：得到数据集，此时数据集里每一个样本是一个 经过 <strong>Field声明的预处理</strong> 预处理后的 wordlist</li><li>建立vocab：在这一步建立词汇表，词向量(word embeddings)</li><li>构造迭代器：构造迭代器，用来分批次训练模型</li></ol><h1 id="1-下载数据："><a href="#1-下载数据：" class="headerlink" title="1. 下载数据："></a>1. 下载数据：</h1><p><a href="https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only/data" target="_blank" rel="noopener">kaggle：Movie Review Sentiment Analysis (Kernels Only)</a><br>train.tsv contains the phrases and their associated sentiment labels. We have additionally provided a SentenceId so that you can track which phrases belong to a single sentence.</p><p>test.tsv contains just phrases. You must assign a sentiment label to each phrase.</p><p>The sentiment labels are:<br>0 - negative<br>1 - somewhat negative<br>2 - neutral<br>3 - somewhat positive<br>4 - positive</p><p>下载得到：train.tsv和test.tsv</p><h2 id="读取文件，查看文件"><a href="#读取文件，查看文件" class="headerlink" title="读取文件，查看文件"></a>读取文件，查看文件</h2><pre><code>import pandas as pddata = pd.read_csv('train.tsv', sep='\t')test = pd.read_csv('test.tsv', sep='\t')</code></pre><h3 id="train-tsv"><a href="#train-tsv" class="headerlink" title="train.tsv"></a>train.tsv</h3><pre><code>data[:5]</code></pre><p><img src="https://img-blog.csdnimg.cn/20190619123238831.png" alt=""></p><h3 id="test-tsv"><a href="#test-tsv" class="headerlink" title="test.tsv"></a>test.tsv</h3><pre><code>test[:5]</code></pre><p><img src="https://img-blog.csdnimg.cn/20190619123248113.png" alt=""></p><h1 id="2-划分验证集"><a href="#2-划分验证集" class="headerlink" title="2. 划分验证集"></a>2. 划分验证集</h1><pre><code>from sklearn.model_selection import train_test_split# create train and validation set train, val = train_test_split(data, test_size=0.2)train.to_csv("train.csv", index=False)val.to_csv("val.csv", index=False)</code></pre><h1 id="3-定义Field"><a href="#3-定义Field" class="headerlink" title="3. 定义Field"></a>3. 定义Field</h1><p>首先导入需要的包和定义pytorch张量使用的DEVICE</p><pre><code>import spacyimport torchfrom torchtext import data, datasetsfrom torchtext.vocab import Vectorsfrom torch.nn import initDEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")</code></pre><p>Torchtext采用了一种声明式的方法来加载数据：你来告诉Torchtext你希望的数据是什么样子的，剩下的由torchtext来处理。<br>实现这种声明的是Field，Field确定了一种你想要怎么去处理数据。<br>data.Field(…)</p><p>Field的参数如下：</p><ul><li>sequential: Whether the datatype represents sequential data. If False, no tokenization is applied. Default: True.</li><li>use_vocab: Whether to use a Vocab object. If False, the data in this field should already be numerical. Default: True.</li><li>init_token: A token that will be prepended to every example using this field, or None for no initial token. Default: None.</li><li>eos_token: A token that will be appended to every example using this field, or None for no end-of-sentence token. Default: None.</li><li>fix_length: A fixed length that all examples using this field will be padded to, or None for flexible sequence lengths. Default: None.</li><li>dtype: The torch.dtype class that represents a batch of examples of this kind of data. Default: torch.long.</li><li>preprocessing: The Pipeline that will be applied to examples using this field after tokenizing but before numericalizing. Many Datasets replace this attribute with a custom preprocessor. Default: None.</li><li>postprocessing: A Pipeline that will be applied to examples using this field after numericalizing but before the numbers are turned into a Tensor. The pipeline function takes the batch as a list, and the field’s Vocab. Default: None.</li><li>lower: Whether to lowercase the text in this field. Default: False.</li><li>tokenize: The function used to tokenize strings using this field into sequential examples. If “spacy”, the SpaCy tokenizer is used. If a non-serializable function is passed as an argument, the field will not be able to be serialized. Default: string.split.</li><li>tokenizer_language: The language of the tokenizer to be constructed. Various languages currently supported only in SpaCy.</li><li>include_lengths: Whether to return a tuple of a padded minibatch and a list containing the lengths of each examples, or just a padded minibatch. Default: False.</li><li>batch_first: Whether to produce tensors with the batch dimension first. Default: False.</li><li>pad_token: The string token used as padding. Default: “<pad>“.</pad></li><li>unk_token: The string token used to represent OOV words. Default: “<unk>“.</unk></li><li>pad_first: Do the padding of the sequence at the beginning. Default: False.</li><li>truncate_first: Do the truncating of the sequence at the beginning. Default: False</li><li>stop_words: Tokens to discard during the preprocessing step. Default: None</li><li>is_target: Whether this field is a target variable. Affects iteration over batches. Default: False</li></ul><p><strong>例：</strong></p><pre><code>spacy_en = spacy.load('en')def tokenizer(text): # create a tokenizer function    """    定义分词操作    """    return [tok.text for tok in spacy_en.tokenizer(text)]"""field在默认的情况下都期望一个输入是一组单词的序列，并且将单词映射成整数。这个映射被称为vocab。如果一个field已经被数字化了并且不需要被序列化，可以将参数设置为use_vocab=False以及sequential=False。"""LABEL = data.Field(sequential=False, use_vocab=False)TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True)</code></pre><h1 id="4-定义Dataset"><a href="#4-定义Dataset" class="headerlink" title="4. 定义Dataset"></a>4. 定义Dataset</h1><p>The fields知道当给定原始数据的时候要做什么。现在，我们需要告诉fields它需要处理什么样的数据。这个功能利用Datasets来实现。</p><p>Torchtext有大量内置的<a href="https://torchtext.readthedocs.io/en/latest/datasets.html" target="_blank" rel="noopener">Datasets</a>去处理各种数据格式。</p><p><strong>TabularDataset官网介绍: Defines a Dataset of columns stored in CSV, TSV, or JSON format.</strong></p><p>对于csv/tsv类型的文件，TabularDataset很容易进行处理，故我们选它来生成Dataset</p><pre><code>"""我们不需要 'PhraseId' 和 'SentenceId'这两列, 所以我们给他们的field传递 None如果你的数据有列名，如我们这里的'Phrase','Sentiment',...设置skip_header=True,不然它会把列名也当一个数据处理"""train,val = data.TabularDataset.splits(        path='.', train='train.csv',validation='val.csv', format='csv',skip_header=True,        fields=[('PhraseId',None),('SentenceId',None),('Phrase', TEXT), ('Sentiment', LABEL)])test = data.TabularDataset('test.tsv', format='tsv',skip_header=True,        fields=[('PhraseId',None),('SentenceId',None),('Phrase', TEXT)])</code></pre><p><strong>注意：传入的(name, field)必须与列的顺序相同。</strong></p><p>查看生成的dataset：</p><pre><code>print(train[5])print(train[5].__dict__.keys())print(train[5].Phrase,train[0].Sentiment)</code></pre><p>输出：<br><img src="https://img-blog.csdnimg.cn/201906191233142.png" alt=""></p><h1 id="5-建立vocab"><a href="#5-建立vocab" class="headerlink" title="5. 建立vocab"></a>5. 建立vocab</h1><p>我们可以看到第6行的输入，它是一个Example对象。Example对象绑定了一行中的所有属性，可以看到，句子已经被分词了，但是没有转化为数字。</p><p>这是因为我们还没有建立vocab，我们将在下一步建立vocab。</p><p>Torchtext可以将词转化为数字，但是它需要被告知需要被处理的全部范围的词。我们可以用下面这行代码：</p><pre><code>TEXT.build_vocab(train, vectors='glove.6B.100d')#, max_size=30000)# 当 corpus 中有的 token 在 vectors 中不存在时 的初始化方式.TEXT.vocab.vectors.unk_init = init.xavier_uniform</code></pre><p>这行代码使得 Torchtext遍历<strong>训练集</strong>中的绑定TEXT field的数据，将单词注册到vocabulary，并自动构建embedding矩阵。</p><p><strong>‘glove.6B.100d’ 为torchtext支持的词向量名字，第一次使用是会自动下载并保存在当前目录的 .vector_cache里面。</strong></p><p><strong>torchtext支持的词向量</strong></p><ul><li>charngram.100d</li><li>fasttext.en.300d</li><li>fasttext.simple.300d</li><li>glove.42B.300d</li><li>glove.840B.300d</li><li>glove.twitter.27B.25d</li><li>glove.twitter.27B.50d</li><li>glove.twitter.27B.100d</li><li>glove.twitter.27B.200d</li><li>glove.6B.50d</li><li>glove.6B.100d</li><li>glove.6B.200d</li><li>glove.6B.300d</li></ul><p><strong>例：</strong></p><p>如果打算使用fasttext.en.300d词向量，只需把上面的代码里的vector=’…’里面的词向量名字换一下即可，具体如下：</p><pre><code>TEXT.build_vocab(train, vectors='fasttext.en.300d')</code></pre><p>到这一步，我们已经可以把<strong>词转为数字，数字转为词，词转为词向量</strong>了</p><pre><code>print(TEXT.vocab.itos[1510])print(TEXT.vocab.stoi['bore'])# 词向量矩阵: TEXT.vocab.vectorsprint(TEXT.vocab.vectors.shape)word_vec = TEXT.vocab.vectors[TEXT.vocab.stoi['bore']]print(word_vec.shape)print(word_vec)</code></pre><p>输出：<br><img src="https://img-blog.csdnimg.cn/20190619123330249.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="6-构造迭代器"><a href="#6-构造迭代器" class="headerlink" title="6. 构造迭代器"></a>6. 构造迭代器</h1><p>我们日常使用pytorch训练网络时，每次训练都是输入一个batch，那么，我们怎么把前面得到的dataset转为迭代器，然后遍历迭代器获取batch输入呢？下面将介绍torchtext时怎么实现这一功能的。</p><p>和Dataset一样，torchtext有大量内置的迭代器，我们这里选择的是BucketIterator，官网对它的介绍如下：</p><ul><li>Defines an iterator that batches examples of similar lengths together.</li><li>Minimizes amount of padding needed while producing freshly shuffled batches for each new epoch. </li></ul><pre><code>train_iter = data.BucketIterator(train, batch_size=128, sort_key=lambda x: len(x.Phrase),                                  shuffle=True,device=DEVICE)val_iter = data.BucketIterator(val, batch_size=128, sort_key=lambda x: len(x.Phrase),                                  shuffle=True,device=DEVICE)# 在 test_iter , sort一定要设置成 False, 要不然会被 torchtext 搞乱样本顺序test_iter = data.Iterator(dataset=test, batch_size=128, train=False,                          sort=False, device=DEVICE)</code></pre><h2 id="迭代器使用"><a href="#迭代器使用" class="headerlink" title="迭代器使用"></a>迭代器使用</h2><h3 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h3><pre><code>batch = next(iter(train_iter))data = batch.Phraselabel = batch.Sentimentprint(batch.Phrase.shape)print(batch.Phrase)</code></pre><p>输出结果：<br><img src="https://img-blog.csdnimg.cn/20190619123347960.png" alt=""><br>可以发现，它输出的是word index，后面的128是batch size</p><h3 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h3><pre><code>for batch in train_iter:    data = batch.Phrase    label = batch.Sentiment</code></pre><h1 id="7-完整代码"><a href="#7-完整代码" class="headerlink" title="7. 完整代码"></a>7. 完整代码</h1><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> spacy<span class="token keyword">import</span> torch<span class="token keyword">from</span> torchtext <span class="token keyword">import</span> data<span class="token punctuation">,</span> datasets<span class="token keyword">from</span> torchtext<span class="token punctuation">.</span>vocab <span class="token keyword">import</span> Vectors<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> init<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pdDEVICE <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span>data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">'train.tsv'</span><span class="token punctuation">,</span> sep<span class="token operator">=</span><span class="token string">'\t'</span><span class="token punctuation">)</span>test <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">'test.tsv'</span><span class="token punctuation">,</span> sep<span class="token operator">=</span><span class="token string">'\t'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># create train and validation set </span>train<span class="token punctuation">,</span> val <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>data<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span>train<span class="token punctuation">.</span>to_csv<span class="token punctuation">(</span><span class="token string">"train.csv"</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>val<span class="token punctuation">.</span>to_csv<span class="token punctuation">(</span><span class="token string">"val.csv"</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>spacy_en <span class="token operator">=</span> spacy<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'en'</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">tokenizer</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># create a tokenizer function</span>    <span class="token keyword">return</span> <span class="token punctuation">[</span>tok<span class="token punctuation">.</span>text <span class="token keyword">for</span> tok <span class="token keyword">in</span> spacy_en<span class="token punctuation">.</span>tokenizer<span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># Field</span>TEXT <span class="token operator">=</span> data<span class="token punctuation">.</span>Field<span class="token punctuation">(</span>sequential<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> tokenize<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span> lower<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>LABEL <span class="token operator">=</span> data<span class="token punctuation">.</span>Field<span class="token punctuation">(</span>sequential<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> use_vocab<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Dataset</span>train<span class="token punctuation">,</span>val <span class="token operator">=</span> data<span class="token punctuation">.</span>TabularDataset<span class="token punctuation">.</span>splits<span class="token punctuation">(</span>        path<span class="token operator">=</span><span class="token string">'.'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token string">'train.csv'</span><span class="token punctuation">,</span>validation<span class="token operator">=</span><span class="token string">'val.csv'</span><span class="token punctuation">,</span> format<span class="token operator">=</span><span class="token string">'csv'</span><span class="token punctuation">,</span>skip_header<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>        fields<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'PhraseId'</span><span class="token punctuation">,</span>None<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token string">'SentenceId'</span><span class="token punctuation">,</span>None<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token string">'Phrase'</span><span class="token punctuation">,</span> TEXT<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'Sentiment'</span><span class="token punctuation">,</span> LABEL<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>test <span class="token operator">=</span> data<span class="token punctuation">.</span>TabularDataset<span class="token punctuation">(</span><span class="token string">'test.tsv'</span><span class="token punctuation">,</span> format<span class="token operator">=</span><span class="token string">'tsv'</span><span class="token punctuation">,</span>skip_header<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>        fields<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'PhraseId'</span><span class="token punctuation">,</span>None<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token string">'SentenceId'</span><span class="token punctuation">,</span>None<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token string">'Phrase'</span><span class="token punctuation">,</span> TEXT<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># build vocab</span>TEXT<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train<span class="token punctuation">,</span> vectors<span class="token operator">=</span><span class="token string">'glove.6B.100d'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#, max_size=30000)</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>vectors<span class="token punctuation">.</span>unk_init <span class="token operator">=</span> init<span class="token punctuation">.</span>xavier_uniform<span class="token comment" spellcheck="true"># Iterator</span>train_iter <span class="token operator">=</span> data<span class="token punctuation">.</span>BucketIterator<span class="token punctuation">(</span>train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> sort_key<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> len<span class="token punctuation">(</span>x<span class="token punctuation">.</span>Phrase<span class="token punctuation">)</span><span class="token punctuation">,</span>                                  shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>device<span class="token operator">=</span>DEVICE<span class="token punctuation">)</span>val_iter <span class="token operator">=</span> data<span class="token punctuation">.</span>BucketIterator<span class="token punctuation">(</span>val<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> sort_key<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> len<span class="token punctuation">(</span>x<span class="token punctuation">.</span>Phrase<span class="token punctuation">)</span><span class="token punctuation">,</span>                                  shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>device<span class="token operator">=</span>DEVICE<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 在 test_iter , sort一定要设置成 False, 要不然会被 torchtext 搞乱样本顺序</span>test_iter <span class="token operator">=</span> data<span class="token punctuation">.</span>Iterator<span class="token punctuation">(</span>dataset<span class="token operator">=</span>test<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>                          sort<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> device<span class="token operator">=</span>DEVICE<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""由于目的是学习torchtext的使用，所以只定义了一个简单模型"""</span>len_vocab <span class="token operator">=</span> len<span class="token punctuation">(</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">Enet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Enet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>len_vocab<span class="token punctuation">,</span><span class="token number">100</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>lstm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span>batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#,bidirectional=True)</span>        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        batch_size<span class="token punctuation">,</span>seq_num <span class="token operator">=</span> x<span class="token punctuation">.</span>shape        vec <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        out<span class="token punctuation">,</span> <span class="token punctuation">(</span>hn<span class="token punctuation">,</span> cn<span class="token punctuation">)</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>lstm<span class="token punctuation">(</span>vec<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>out<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        out <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>out<span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> outmodel <span class="token operator">=</span> Enet<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""将前面生成的词向量矩阵拷贝到模型的embedding层这样就自动的可以将输入的word index转为词向量"""</span>model<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>copy_<span class="token punctuation">(</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>vectors<span class="token punctuation">)</span>   model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>DEVICE<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 训练</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#,lr=0.000001)</span>n_epoch <span class="token operator">=</span> <span class="token number">20</span>best_val_acc <span class="token operator">=</span> <span class="token number">0</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> batch_idx<span class="token punctuation">,</span> batch <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>train_iter<span class="token punctuation">)</span><span class="token punctuation">:</span>        data <span class="token operator">=</span> batch<span class="token punctuation">.</span>Phrase        target <span class="token operator">=</span> batch<span class="token punctuation">.</span>Sentiment        target <span class="token operator">=</span> torch<span class="token punctuation">.</span>sparse<span class="token punctuation">.</span>torch<span class="token punctuation">.</span>eye<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>index_select<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> index<span class="token operator">=</span>target<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data<span class="token punctuation">)</span>        target <span class="token operator">=</span> target<span class="token punctuation">.</span>to<span class="token punctuation">(</span>DEVICE<span class="token punctuation">)</span>        data <span class="token operator">=</span> data<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        out <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">)</span>        loss <span class="token operator">=</span> <span class="token operator">-</span>target<span class="token operator">*</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>out<span class="token punctuation">)</span><span class="token operator">-</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>target<span class="token punctuation">)</span><span class="token operator">*</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>out<span class="token punctuation">)</span>        loss <span class="token operator">=</span> loss<span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>batch_idx<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span><span class="token number">200</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            _<span class="token punctuation">,</span>y_pre <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>out<span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>            acc <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>y_pre <span class="token operator">==</span> batch<span class="token punctuation">.</span>Sentiment<span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'epoch: %d \t batch_idx : %d \t loss: %.4f \t train acc: %.4f'</span>                  <span class="token operator">%</span><span class="token punctuation">(</span>epoch<span class="token punctuation">,</span>batch_idx<span class="token punctuation">,</span>loss<span class="token punctuation">,</span>acc<span class="token punctuation">)</span><span class="token punctuation">)</span>    val_accs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> batch_idx<span class="token punctuation">,</span> batch <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>val_iter<span class="token punctuation">)</span><span class="token punctuation">:</span>        data <span class="token operator">=</span> batch<span class="token punctuation">.</span>Phrase        target <span class="token operator">=</span> batch<span class="token punctuation">.</span>Sentiment        target <span class="token operator">=</span> torch<span class="token punctuation">.</span>sparse<span class="token punctuation">.</span>torch<span class="token punctuation">.</span>eye<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>index_select<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> index<span class="token operator">=</span>target<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data<span class="token punctuation">)</span>        target <span class="token operator">=</span> target<span class="token punctuation">.</span>to<span class="token punctuation">(</span>DEVICE<span class="token punctuation">)</span>        data <span class="token operator">=</span> data<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span>        out <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">)</span>        _<span class="token punctuation">,</span>y_pre <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>out<span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        acc <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>y_pre <span class="token operator">==</span> batch<span class="token punctuation">.</span>Sentiment<span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        val_accs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>acc<span class="token punctuation">)</span>    acc <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>val_accs<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> acc <span class="token operator">&gt;</span> best_val_acc<span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'val acc : %.4f &gt; %.4f saving model'</span><span class="token operator">%</span><span class="token punctuation">(</span>acc<span class="token punctuation">,</span>best_val_acc<span class="token punctuation">)</span><span class="token punctuation">)</span>        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'params.pkl'</span><span class="token punctuation">)</span>        best_val_acc <span class="token operator">=</span> acc    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'val acc: %.4f'</span><span class="token operator">%</span><span class="token punctuation">(</span>acc<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h1 id="8-参考"><a href="#8-参考" class="headerlink" title="8. 参考"></a>8. 参考</h1><ul><li><a href="https://zhuanlan.zhihu.com/p/65833208" target="_blank" rel="noopener">pytorch学习笔记—Torchtext</a></li><li><a href="https://zhuanlan.zhihu.com/p/34722385" target="_blank" rel="noopener">使用 torchtext 做 Toxic Comment Classification 比赛的数据预处理</a></li><li><a href="https://towardsdatascience.com/how-to-use-torchtext-for-neural-machine-translation-plus-hack-to-make-it-5x-faster-77f3884d95" target="_blank" rel="noopener">How to use TorchText for neural machine translation, plus hack to make it 5x faster</a></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> torchtext </tag>
            
            <tag> python </tag>
            
            <tag> nlp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2020/02/29/hello-world/"/>
      <url>/2020/02/29/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-bash"><code class="language-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-bash"><code class="language-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-bash"><code class="language-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
