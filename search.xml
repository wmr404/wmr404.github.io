<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Mac OS VMware Fusion Centos6.5虚拟机网络设置</title>
      <link href="/2020/02/29/vmware/"/>
      <url>/2020/02/29/vmware/</url>
      
        <content type="html"><![CDATA[<h1 id="1-安装vmware虚拟机"><a href="#1-安装vmware虚拟机" class="headerlink" title="1. 安装vmware虚拟机"></a>1. 安装vmware虚拟机</h1><p>安装vmware虚拟机，并新建一个centos 64位的虚拟机</p><h1 id="2-设置虚拟机网络模式"><a href="#2-设置虚拟机网络模式" class="headerlink" title="2. 设置虚拟机网络模式"></a>2. 设置虚拟机网络模式</h1><p><img src="https://img-blog.csdnimg.cn/20200210222413551.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="3-查看vmware的网关和掩码"><a href="#3-查看vmware的网关和掩码" class="headerlink" title="3. 查看vmware的网关和掩码"></a>3. 查看vmware的网关和掩码</h1><p>在Mac电脑的终端输入：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">cat</span> /Library/Preferences/VMware\ Fusion/vmnet8/nat.conf</code></pre><p>输出结果如下：<br><img src="https://img-blog.csdnimg.cn/20200210222624261.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt=""><br>这里的ip和netmask即为vmware虚拟机的网关和掩码</p><pre class=" language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># NAT gateway address</span>ip <span class="token operator">=</span> 172.16.143.2netmask <span class="token operator">=</span> 255.255.255.0</code></pre><p>ip和netmask后面配置centos虚拟机的网络时分别对于网关和掩码。</p><h1 id="4-配置centos虚拟机的网络"><a href="#4-配置centos虚拟机的网络" class="headerlink" title="4. 配置centos虚拟机的网络"></a>4. 配置centos虚拟机的网络</h1><p>在centos虚拟机的终端输入：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">vi</span> /etc/sysconfig/network-scripts/ifcfg-eth0</code></pre><p>然后：</p><ul><li>删除UUID和MAC地址</li><li>ONBOOT=yes</li><li>BOOTPROTO=static</li><li>IPADDR=172.16.143.101</li><li>NETMASK=255.255.255.0</li><li>GATEWAY=172.16.143.2</li><li>DNS1=172.16.143.2</li></ul><p>保存并退出，然后在centos虚拟机的终端输入：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">service</span> network restart</code></pre><p>ping 一下百度看是否能ping通：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">ping</span> www.baidu.com</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> vmware </tag>
            
            <tag> macos </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Torchtext使用教程</title>
      <link href="/2020/02/29/torchtext-use/"/>
      <url>/2020/02/29/torchtext-use/</url>
      
        <content type="html"><![CDATA[<h1 id="Torchtext使用教程"><a href="#Torchtext使用教程" class="headerlink" title="Torchtext使用教程"></a>Torchtext使用教程</h1><h2 id="主要内容："><a href="#主要内容：" class="headerlink" title="主要内容："></a>主要内容：</h2><ul><li>如何使用torchtext建立语料库</li><li>如何使用torchtext将词转下标，下标转词，词转词向量</li><li>如何建立相应的迭代器</li></ul><h2 id="torchtext预处理流程："><a href="#torchtext预处理流程：" class="headerlink" title="torchtext预处理流程："></a>torchtext预处理流程：</h2><ol><li>定义Field：声明如何处理数据</li><li>定义Dataset：得到数据集，此时数据集里每一个样本是一个 经过 <strong>Field声明的预处理</strong> 预处理后的 wordlist</li><li>建立vocab：在这一步建立词汇表，词向量(word embeddings)</li><li>构造迭代器：构造迭代器，用来分批次训练模型</li></ol><h1 id="1-下载数据："><a href="#1-下载数据：" class="headerlink" title="1. 下载数据："></a>1. 下载数据：</h1><p><a href="https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only/data" target="_blank" rel="noopener">kaggle：Movie Review Sentiment Analysis (Kernels Only)</a><br>train.tsv contains the phrases and their associated sentiment labels. We have additionally provided a SentenceId so that you can track which phrases belong to a single sentence.</p><p>test.tsv contains just phrases. You must assign a sentiment label to each phrase.</p><p>The sentiment labels are:<br>0 - negative<br>1 - somewhat negative<br>2 - neutral<br>3 - somewhat positive<br>4 - positive</p><p>下载得到：train.tsv和test.tsv</p><h2 id="读取文件，查看文件"><a href="#读取文件，查看文件" class="headerlink" title="读取文件，查看文件"></a>读取文件，查看文件</h2><pre><code>import pandas as pddata = pd.read_csv('train.tsv', sep='\t')test = pd.read_csv('test.tsv', sep='\t')</code></pre><h3 id="train-tsv"><a href="#train-tsv" class="headerlink" title="train.tsv"></a>train.tsv</h3><pre><code>data[:5]</code></pre><p><img src="https://img-blog.csdnimg.cn/20190619123238831.png" alt=""></p><h3 id="test-tsv"><a href="#test-tsv" class="headerlink" title="test.tsv"></a>test.tsv</h3><pre><code>test[:5]</code></pre><p><img src="https://img-blog.csdnimg.cn/20190619123248113.png" alt=""></p><h1 id="2-划分验证集"><a href="#2-划分验证集" class="headerlink" title="2. 划分验证集"></a>2. 划分验证集</h1><pre><code>from sklearn.model_selection import train_test_split# create train and validation set train, val = train_test_split(data, test_size=0.2)train.to_csv("train.csv", index=False)val.to_csv("val.csv", index=False)</code></pre><h1 id="3-定义Field"><a href="#3-定义Field" class="headerlink" title="3. 定义Field"></a>3. 定义Field</h1><p>首先导入需要的包和定义pytorch张量使用的DEVICE</p><pre><code>import spacyimport torchfrom torchtext import data, datasetsfrom torchtext.vocab import Vectorsfrom torch.nn import initDEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")</code></pre><p>Torchtext采用了一种声明式的方法来加载数据：你来告诉Torchtext你希望的数据是什么样子的，剩下的由torchtext来处理。<br>实现这种声明的是Field，Field确定了一种你想要怎么去处理数据。<br>data.Field(…)</p><p>Field的参数如下：</p><ul><li>sequential: Whether the datatype represents sequential data. If False, no tokenization is applied. Default: True.</li><li>use_vocab: Whether to use a Vocab object. If False, the data in this field should already be numerical. Default: True.</li><li>init_token: A token that will be prepended to every example using this field, or None for no initial token. Default: None.</li><li>eos_token: A token that will be appended to every example using this field, or None for no end-of-sentence token. Default: None.</li><li>fix_length: A fixed length that all examples using this field will be padded to, or None for flexible sequence lengths. Default: None.</li><li>dtype: The torch.dtype class that represents a batch of examples of this kind of data. Default: torch.long.</li><li>preprocessing: The Pipeline that will be applied to examples using this field after tokenizing but before numericalizing. Many Datasets replace this attribute with a custom preprocessor. Default: None.</li><li>postprocessing: A Pipeline that will be applied to examples using this field after numericalizing but before the numbers are turned into a Tensor. The pipeline function takes the batch as a list, and the field’s Vocab. Default: None.</li><li>lower: Whether to lowercase the text in this field. Default: False.</li><li>tokenize: The function used to tokenize strings using this field into sequential examples. If “spacy”, the SpaCy tokenizer is used. If a non-serializable function is passed as an argument, the field will not be able to be serialized. Default: string.split.</li><li>tokenizer_language: The language of the tokenizer to be constructed. Various languages currently supported only in SpaCy.</li><li>include_lengths: Whether to return a tuple of a padded minibatch and a list containing the lengths of each examples, or just a padded minibatch. Default: False.</li><li>batch_first: Whether to produce tensors with the batch dimension first. Default: False.</li><li>pad_token: The string token used as padding. Default: “<pad>“.</pad></li><li>unk_token: The string token used to represent OOV words. Default: “<unk>“.</unk></li><li>pad_first: Do the padding of the sequence at the beginning. Default: False.</li><li>truncate_first: Do the truncating of the sequence at the beginning. Default: False</li><li>stop_words: Tokens to discard during the preprocessing step. Default: None</li><li>is_target: Whether this field is a target variable. Affects iteration over batches. Default: False</li></ul><p><strong>例：</strong></p><pre><code>spacy_en = spacy.load('en')def tokenizer(text): # create a tokenizer function    """    定义分词操作    """    return [tok.text for tok in spacy_en.tokenizer(text)]"""field在默认的情况下都期望一个输入是一组单词的序列，并且将单词映射成整数。这个映射被称为vocab。如果一个field已经被数字化了并且不需要被序列化，可以将参数设置为use_vocab=False以及sequential=False。"""LABEL = data.Field(sequential=False, use_vocab=False)TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True)</code></pre><h1 id="4-定义Dataset"><a href="#4-定义Dataset" class="headerlink" title="4. 定义Dataset"></a>4. 定义Dataset</h1><p>The fields知道当给定原始数据的时候要做什么。现在，我们需要告诉fields它需要处理什么样的数据。这个功能利用Datasets来实现。</p><p>Torchtext有大量内置的<a href="https://torchtext.readthedocs.io/en/latest/datasets.html" target="_blank" rel="noopener">Datasets</a>去处理各种数据格式。</p><p><strong>TabularDataset官网介绍: Defines a Dataset of columns stored in CSV, TSV, or JSON format.</strong></p><p>对于csv/tsv类型的文件，TabularDataset很容易进行处理，故我们选它来生成Dataset</p><pre><code>"""我们不需要 'PhraseId' 和 'SentenceId'这两列, 所以我们给他们的field传递 None如果你的数据有列名，如我们这里的'Phrase','Sentiment',...设置skip_header=True,不然它会把列名也当一个数据处理"""train,val = data.TabularDataset.splits(        path='.', train='train.csv',validation='val.csv', format='csv',skip_header=True,        fields=[('PhraseId',None),('SentenceId',None),('Phrase', TEXT), ('Sentiment', LABEL)])test = data.TabularDataset('test.tsv', format='tsv',skip_header=True,        fields=[('PhraseId',None),('SentenceId',None),('Phrase', TEXT)])</code></pre><p><strong>注意：传入的(name, field)必须与列的顺序相同。</strong></p><p>查看生成的dataset：</p><pre><code>print(train[5])print(train[5].__dict__.keys())print(train[5].Phrase,train[0].Sentiment)</code></pre><p>输出：<br><img src="https://img-blog.csdnimg.cn/201906191233142.png" alt=""></p><h1 id="5-建立vocab"><a href="#5-建立vocab" class="headerlink" title="5. 建立vocab"></a>5. 建立vocab</h1><p>我们可以看到第6行的输入，它是一个Example对象。Example对象绑定了一行中的所有属性，可以看到，句子已经被分词了，但是没有转化为数字。</p><p>这是因为我们还没有建立vocab，我们将在下一步建立vocab。</p><p>Torchtext可以将词转化为数字，但是它需要被告知需要被处理的全部范围的词。我们可以用下面这行代码：</p><pre><code>TEXT.build_vocab(train, vectors='glove.6B.100d')#, max_size=30000)# 当 corpus 中有的 token 在 vectors 中不存在时 的初始化方式.TEXT.vocab.vectors.unk_init = init.xavier_uniform</code></pre><p>这行代码使得 Torchtext遍历<strong>训练集</strong>中的绑定TEXT field的数据，将单词注册到vocabulary，并自动构建embedding矩阵。</p><p><strong>‘glove.6B.100d’ 为torchtext支持的词向量名字，第一次使用是会自动下载并保存在当前目录的 .vector_cache里面。</strong></p><p><strong>torchtext支持的词向量</strong></p><ul><li>charngram.100d</li><li>fasttext.en.300d</li><li>fasttext.simple.300d</li><li>glove.42B.300d</li><li>glove.840B.300d</li><li>glove.twitter.27B.25d</li><li>glove.twitter.27B.50d</li><li>glove.twitter.27B.100d</li><li>glove.twitter.27B.200d</li><li>glove.6B.50d</li><li>glove.6B.100d</li><li>glove.6B.200d</li><li>glove.6B.300d</li></ul><p><strong>例：</strong></p><p>如果打算使用fasttext.en.300d词向量，只需把上面的代码里的vector=’…’里面的词向量名字换一下即可，具体如下：</p><pre><code>TEXT.build_vocab(train, vectors='fasttext.en.300d')</code></pre><p>到这一步，我们已经可以把<strong>词转为数字，数字转为词，词转为词向量</strong>了</p><pre><code>print(TEXT.vocab.itos[1510])print(TEXT.vocab.stoi['bore'])# 词向量矩阵: TEXT.vocab.vectorsprint(TEXT.vocab.vectors.shape)word_vec = TEXT.vocab.vectors[TEXT.vocab.stoi['bore']]print(word_vec.shape)print(word_vec)</code></pre><p>输出：<br><img src="https://img-blog.csdnimg.cn/20190619123330249.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pXb3N3aW4=,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="6-构造迭代器"><a href="#6-构造迭代器" class="headerlink" title="6. 构造迭代器"></a>6. 构造迭代器</h1><p>我们日常使用pytorch训练网络时，每次训练都是输入一个batch，那么，我们怎么把前面得到的dataset转为迭代器，然后遍历迭代器获取batch输入呢？下面将介绍torchtext时怎么实现这一功能的。</p><p>和Dataset一样，torchtext有大量内置的迭代器，我们这里选择的是BucketIterator，官网对它的介绍如下：</p><ul><li>Defines an iterator that batches examples of similar lengths together.</li><li>Minimizes amount of padding needed while producing freshly shuffled batches for each new epoch. </li></ul><pre><code>train_iter = data.BucketIterator(train, batch_size=128, sort_key=lambda x: len(x.Phrase),                                  shuffle=True,device=DEVICE)val_iter = data.BucketIterator(val, batch_size=128, sort_key=lambda x: len(x.Phrase),                                  shuffle=True,device=DEVICE)# 在 test_iter , sort一定要设置成 False, 要不然会被 torchtext 搞乱样本顺序test_iter = data.Iterator(dataset=test, batch_size=128, train=False,                          sort=False, device=DEVICE)</code></pre><h2 id="迭代器使用"><a href="#迭代器使用" class="headerlink" title="迭代器使用"></a>迭代器使用</h2><h3 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h3><pre><code>batch = next(iter(train_iter))data = batch.Phraselabel = batch.Sentimentprint(batch.Phrase.shape)print(batch.Phrase)</code></pre><p>输出结果：<br><img src="https://img-blog.csdnimg.cn/20190619123347960.png" alt=""><br>可以发现，它输出的是word index，后面的128是batch size</p><h3 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h3><pre><code>for batch in train_iter:    data = batch.Phrase    label = batch.Sentiment</code></pre><h1 id="7-完整代码"><a href="#7-完整代码" class="headerlink" title="7. 完整代码"></a>7. 完整代码</h1><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> spacy<span class="token keyword">import</span> torch<span class="token keyword">from</span> torchtext <span class="token keyword">import</span> data<span class="token punctuation">,</span> datasets<span class="token keyword">from</span> torchtext<span class="token punctuation">.</span>vocab <span class="token keyword">import</span> Vectors<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> init<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pdDEVICE <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span>data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">'train.tsv'</span><span class="token punctuation">,</span> sep<span class="token operator">=</span><span class="token string">'\t'</span><span class="token punctuation">)</span>test <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">'test.tsv'</span><span class="token punctuation">,</span> sep<span class="token operator">=</span><span class="token string">'\t'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># create train and validation set </span>train<span class="token punctuation">,</span> val <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>data<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span>train<span class="token punctuation">.</span>to_csv<span class="token punctuation">(</span><span class="token string">"train.csv"</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>val<span class="token punctuation">.</span>to_csv<span class="token punctuation">(</span><span class="token string">"val.csv"</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>spacy_en <span class="token operator">=</span> spacy<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'en'</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">tokenizer</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment" spellcheck="true"># create a tokenizer function</span>    <span class="token keyword">return</span> <span class="token punctuation">[</span>tok<span class="token punctuation">.</span>text <span class="token keyword">for</span> tok <span class="token keyword">in</span> spacy_en<span class="token punctuation">.</span>tokenizer<span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># Field</span>TEXT <span class="token operator">=</span> data<span class="token punctuation">.</span>Field<span class="token punctuation">(</span>sequential<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> tokenize<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span> lower<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>LABEL <span class="token operator">=</span> data<span class="token punctuation">.</span>Field<span class="token punctuation">(</span>sequential<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> use_vocab<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Dataset</span>train<span class="token punctuation">,</span>val <span class="token operator">=</span> data<span class="token punctuation">.</span>TabularDataset<span class="token punctuation">.</span>splits<span class="token punctuation">(</span>        path<span class="token operator">=</span><span class="token string">'.'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token string">'train.csv'</span><span class="token punctuation">,</span>validation<span class="token operator">=</span><span class="token string">'val.csv'</span><span class="token punctuation">,</span> format<span class="token operator">=</span><span class="token string">'csv'</span><span class="token punctuation">,</span>skip_header<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>        fields<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'PhraseId'</span><span class="token punctuation">,</span>None<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token string">'SentenceId'</span><span class="token punctuation">,</span>None<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token string">'Phrase'</span><span class="token punctuation">,</span> TEXT<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'Sentiment'</span><span class="token punctuation">,</span> LABEL<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>test <span class="token operator">=</span> data<span class="token punctuation">.</span>TabularDataset<span class="token punctuation">(</span><span class="token string">'test.tsv'</span><span class="token punctuation">,</span> format<span class="token operator">=</span><span class="token string">'tsv'</span><span class="token punctuation">,</span>skip_header<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>        fields<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'PhraseId'</span><span class="token punctuation">,</span>None<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token string">'SentenceId'</span><span class="token punctuation">,</span>None<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token string">'Phrase'</span><span class="token punctuation">,</span> TEXT<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># build vocab</span>TEXT<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train<span class="token punctuation">,</span> vectors<span class="token operator">=</span><span class="token string">'glove.6B.100d'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#, max_size=30000)</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>vectors<span class="token punctuation">.</span>unk_init <span class="token operator">=</span> init<span class="token punctuation">.</span>xavier_uniform<span class="token comment" spellcheck="true"># Iterator</span>train_iter <span class="token operator">=</span> data<span class="token punctuation">.</span>BucketIterator<span class="token punctuation">(</span>train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> sort_key<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> len<span class="token punctuation">(</span>x<span class="token punctuation">.</span>Phrase<span class="token punctuation">)</span><span class="token punctuation">,</span>                                  shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>device<span class="token operator">=</span>DEVICE<span class="token punctuation">)</span>val_iter <span class="token operator">=</span> data<span class="token punctuation">.</span>BucketIterator<span class="token punctuation">(</span>val<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> sort_key<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> len<span class="token punctuation">(</span>x<span class="token punctuation">.</span>Phrase<span class="token punctuation">)</span><span class="token punctuation">,</span>                                  shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>device<span class="token operator">=</span>DEVICE<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 在 test_iter , sort一定要设置成 False, 要不然会被 torchtext 搞乱样本顺序</span>test_iter <span class="token operator">=</span> data<span class="token punctuation">.</span>Iterator<span class="token punctuation">(</span>dataset<span class="token operator">=</span>test<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>                          sort<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> device<span class="token operator">=</span>DEVICE<span class="token punctuation">)</span><span class="token triple-quoted-string string">"""由于目的是学习torchtext的使用，所以只定义了一个简单模型"""</span>len_vocab <span class="token operator">=</span> len<span class="token punctuation">(</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">Enet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Enet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>len_vocab<span class="token punctuation">,</span><span class="token number">100</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>lstm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span>batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#,bidirectional=True)</span>        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        batch_size<span class="token punctuation">,</span>seq_num <span class="token operator">=</span> x<span class="token punctuation">.</span>shape        vec <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        out<span class="token punctuation">,</span> <span class="token punctuation">(</span>hn<span class="token punctuation">,</span> cn<span class="token punctuation">)</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>lstm<span class="token punctuation">(</span>vec<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>out<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        out <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>out<span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> outmodel <span class="token operator">=</span> Enet<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token triple-quoted-string string">"""将前面生成的词向量矩阵拷贝到模型的embedding层这样就自动的可以将输入的word index转为词向量"""</span>model<span class="token punctuation">.</span>embedding<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>copy_<span class="token punctuation">(</span>TEXT<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>vectors<span class="token punctuation">)</span>   model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>DEVICE<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 训练</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#,lr=0.000001)</span>n_epoch <span class="token operator">=</span> <span class="token number">20</span>best_val_acc <span class="token operator">=</span> <span class="token number">0</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> batch_idx<span class="token punctuation">,</span> batch <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>train_iter<span class="token punctuation">)</span><span class="token punctuation">:</span>        data <span class="token operator">=</span> batch<span class="token punctuation">.</span>Phrase        target <span class="token operator">=</span> batch<span class="token punctuation">.</span>Sentiment        target <span class="token operator">=</span> torch<span class="token punctuation">.</span>sparse<span class="token punctuation">.</span>torch<span class="token punctuation">.</span>eye<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>index_select<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> index<span class="token operator">=</span>target<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data<span class="token punctuation">)</span>        target <span class="token operator">=</span> target<span class="token punctuation">.</span>to<span class="token punctuation">(</span>DEVICE<span class="token punctuation">)</span>        data <span class="token operator">=</span> data<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        out <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">)</span>        loss <span class="token operator">=</span> <span class="token operator">-</span>target<span class="token operator">*</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>out<span class="token punctuation">)</span><span class="token operator">-</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>target<span class="token punctuation">)</span><span class="token operator">*</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>out<span class="token punctuation">)</span>        loss <span class="token operator">=</span> loss<span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>batch_idx<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span><span class="token number">200</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            _<span class="token punctuation">,</span>y_pre <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>out<span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>            acc <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>y_pre <span class="token operator">==</span> batch<span class="token punctuation">.</span>Sentiment<span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'epoch: %d \t batch_idx : %d \t loss: %.4f \t train acc: %.4f'</span>                  <span class="token operator">%</span><span class="token punctuation">(</span>epoch<span class="token punctuation">,</span>batch_idx<span class="token punctuation">,</span>loss<span class="token punctuation">,</span>acc<span class="token punctuation">)</span><span class="token punctuation">)</span>    val_accs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> batch_idx<span class="token punctuation">,</span> batch <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>val_iter<span class="token punctuation">)</span><span class="token punctuation">:</span>        data <span class="token operator">=</span> batch<span class="token punctuation">.</span>Phrase        target <span class="token operator">=</span> batch<span class="token punctuation">.</span>Sentiment        target <span class="token operator">=</span> torch<span class="token punctuation">.</span>sparse<span class="token punctuation">.</span>torch<span class="token punctuation">.</span>eye<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>index_select<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> index<span class="token operator">=</span>target<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data<span class="token punctuation">)</span>        target <span class="token operator">=</span> target<span class="token punctuation">.</span>to<span class="token punctuation">(</span>DEVICE<span class="token punctuation">)</span>        data <span class="token operator">=</span> data<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span>        out <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">)</span>        _<span class="token punctuation">,</span>y_pre <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>out<span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        acc <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>y_pre <span class="token operator">==</span> batch<span class="token punctuation">.</span>Sentiment<span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        val_accs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>acc<span class="token punctuation">)</span>    acc <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>val_accs<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> acc <span class="token operator">></span> best_val_acc<span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'val acc : %.4f > %.4f saving model'</span><span class="token operator">%</span><span class="token punctuation">(</span>acc<span class="token punctuation">,</span>best_val_acc<span class="token punctuation">)</span><span class="token punctuation">)</span>        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'params.pkl'</span><span class="token punctuation">)</span>        best_val_acc <span class="token operator">=</span> acc    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'val acc: %.4f'</span><span class="token operator">%</span><span class="token punctuation">(</span>acc<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h1 id="8-参考"><a href="#8-参考" class="headerlink" title="8. 参考"></a>8. 参考</h1><ul><li><a href="https://zhuanlan.zhihu.com/p/65833208" target="_blank" rel="noopener">pytorch学习笔记—Torchtext</a></li><li><a href="https://zhuanlan.zhihu.com/p/34722385" target="_blank" rel="noopener">使用 torchtext 做 Toxic Comment Classification 比赛的数据预处理</a></li><li><a href="https://towardsdatascience.com/how-to-use-torchtext-for-neural-machine-translation-plus-hack-to-make-it-5x-faster-77f3884d95" target="_blank" rel="noopener">How to use TorchText for neural machine translation, plus hack to make it 5x faster</a></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> torchtext </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2020/02/29/hello-world/"/>
      <url>/2020/02/29/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-bash"><code class="language-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-bash"><code class="language-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-bash"><code class="language-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
